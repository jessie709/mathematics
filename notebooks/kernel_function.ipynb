{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are kernels in machine learning and SVM and why do we need them?\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - [What are kernels in machine learning and SVM and why do we need them?](https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them)\n",
    "\n",
    "## Briefly speaking\n",
    "\n",
    "A kernel is a shortcut that help us do certain calculation faster which otherwise would involve computations in higher dimensional space.\n",
    "\n",
    "*简而言之，核函数是一种便捷方法，用于提高计算效率，避免在高维进行计算（因为高维计算的代价极高，甚至无法计算，比如无限维的情况）。*\n",
    "\n",
    "## Mathematical definition\n",
    "\n",
    "$K(x,y) = <\\phi{(x)}, \\phi{(y)}>$. Here $K$ is the kernel function, $x,y$ are $n$ dimensional inputs. $\\phi$ is a map from n-dimension to m-dimension space. $<x,y>$ denotes the dot product. usually $m$ is much large than n.\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Normally calculating $<\\phi{(x)}, \\phi{(y)}>$ requires us to calculate $\\phi{(x)}, \\phi{(y)}$ first, and then do the dot product. These two computation steps can be quite expensive as they involve manipulation in $m$ dimensional space, where $m$ can be a large number. But after all the trouble of going to the high dimensional space, the result of the dot product is really a scalar: we come back to one-dimensional space agein! Now, the question we have is: do we really need to go through all the trouble to get this one number? do we really have to go to the m-dimensional space? The answer is no, if you find a clever kernel.\n",
    "\n",
    "*从更直观的角度来审视，通常计算 $<\\phi{(x)}, \\phi{(y)}>$ 需要首先计算 $\\phi{(x)}, \\phi{(y)}$ 然后再进行点乘操作。当涉及在很高维空间进行相关操作时，代价是极高的。从本质上而言，我们进入高维空间的目的仅仅是为了得到一个标量值（点乘的结果是标量）！我们是否有必要非要到高维空间去计算一个标量数值？答案是，没有必要，如果你能找到一个聪明的核函数的话。*\n",
    "\n",
    "## Simple Example\n",
    "\n",
    "$x=[x_1,x_2,x_3]^T; y=[y_1,y_2,y_3]^T$. Then for the function $\\phi(x) = [x_1x_1,x_1x_2,x_1x_3,x_2x_1,x_2x_2,x_2x_3,x_3x_1,x_3x_2,x_3x_3]^T$, the kernel function is $K(x,y)=(<x,y>)^2$\n",
    "\n",
    "这个列子的意思是，将一个三维空间的向量，通过 $\\phi(x)$ 函数映射到九维空间中去，映射到九维空间的向量的点积结果与在三维空间点积结果的平方相等。这暗示着核函数的作用就是，基于 $\\phi(x)$ 这种映射关系，我们没有必要现将3维映射到哦9维后，在9维空间进行点积操作，而是只需要在3维空间中直接求点积然后再平方即可。\n",
    "\n",
    "为了更加直观的理解，我们填入一些真实的数字：假设 $x=[1,2,3]^T;y=[4,5,6]^T$, 这样：\n",
    "\n",
    "$\\phi(x)=[1,2,3,2,4,6,3,6,9]^T$\n",
    "\n",
    "$\\phi(y)=[16,20,24,20,25,30,24,30,36]^T$\n",
    "\n",
    "$<\\phi(x),\\phi(y)>=16+40+72+40+100+180+72+180+324=1024$\n",
    "\n",
    "现在我们用核函数来计算：\n",
    "\n",
    "$K(x,y)=x \\cdot y=(4+10+18)^2=1024$\n",
    "\n",
    "同样的结果，但计算简单多了。\n",
    "\n",
    "## Additional beauty of Kernel\n",
    "\n",
    "Kernel allow us to stuff in infinite dimension! Sometimes going to higher dimension is not just computationally expensive, but also impossible. $\\phi(x)$ can be a mapping from $n$ dimension to infinite dimension which we may have little idea of how to deal with. Then kernel gives us a wonderful shortcut.\n",
    "\n",
    "*核函数允许我们在无限维做些事情！有时候去到更高维度计算是很昂贵的，甚至是不可行的。将 $n$ 维空间映射到无限维空间，我们几乎不知道该如何处理。幸好，核函数给了我们提供了绝妙的路径。*\n",
    "\n",
    "## Relation to SVM\n",
    "\n",
    "Now how is related to SVM? The idea of SVM is that $y=w\\phi(x)+b$, where $w$ is the weight, $\\phi(x)$ is the feature vector, and $b$ is the bias. if $y>0$, the we classify datum to class 1, else to class 0. We want to find a set of weight and bias such that the margin is maximized. Previous answers mention that kernel makes data linearly separable for SVM. I think a more precise way to put this is, *kernels do not make the data linearly sparable. The feature vector $\\phi(x)$ makes the data linearly separable.* Kernel is to make the calculation process faster and easier, especially when the feature vector $\\phi(x)$ is of very high dimension.\n",
    "\n",
    "## Why it can also be understood as a measure of similarity\n",
    "\n",
    "The inner product means the projection of $phi(x)$ onto $\\phi(y)$. Or colloquially, how much overlap do $x$ and $y$ have in their feature space. In others words, how similar they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
