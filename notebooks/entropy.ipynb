{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's an intuitive way to think of cross entropy\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - [What's an intuitive way to think of cross entropy](https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Information Entropy\n",
    "\n",
    "**Source:**\n",
    "\n",
    ">- [A Gentle Introduction to Information Entropy](https://machinelearningmastery.com/what-is-information-entropy/)\n",
    "\n",
    "## What is information theory\n",
    "\n",
    "Information theory is field of study concerned with quantifying information for communication.\n",
    "\n",
    "*信息论是涉及量化沟通信息的研究领域.*\n",
    "\n",
    "A foundational concept from information is the quantification of the amount of information in things like events, random variables, and distribution.\n",
    "\n",
    "*信息的一个基本概念是对诸如事件, 随机变量和分布的信息量的量化.*\n",
    "\n",
    "> *Why unify information theory and machine learning? Because they are two side of the ame coin. Information theory and machine learning still alone together.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the information for an event\n",
    "\n",
    "Quantifying information is the foundation of field of information theory.\n",
    "\n",
    "The intuition behind quantifying information is the idea of measuring how much surprise there is in an event. Those events that rare (low probability) are more surprising and therefore have more information those events that there are common (high probability).\n",
    "\n",
    "- **Low probability event:** High information (surprising).\n",
    "- **High probability event:** Low information (unsurprising).\n",
    "\n",
    "Rare events are more uncertain or more surprising and require more information to represent them than common events.\n",
    "\n",
    "We can calculate the amount of information there is in an event using the probability of the event. This is called *\"Shannon information\", \"self-information\"*, or simply the *\"information\"*, and can be calculated for a discrete event $x$ as follows:\n",
    "\n",
    "- $information(x) = -\\log_2(p(x))$\n",
    "\n",
    "Where $p(x)$ is the probability of the event $x$.\n",
    "\n",
    "The choice of the 2-base logarithm means that the units of information measure is in bits(binary digits).\n",
    "\n",
    "The calculate of information is often written as $h()$ for example:\n",
    "\n",
    "- $h(x) = -\\log_2(p(x))$\n",
    "\n",
    "The negative sign ensures that the result is always positive or zero.\n",
    "\n",
    "Information will be zero when the probability of an event is $1.0$ or a certainty, e.g. there is no surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this concrete with some examples.\n",
    "\n",
    "Consider a flip of a single fair coin. The probability of heads(and tails) is 0.5. We can calculate the information for flipping a head in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T03:20:12.817600Z",
     "start_time": "2019-11-12T03:20:12.813600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T03:36:42.021001Z",
     "start_time": "2019-11-12T03:36:41.976001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(x)=0.5, information: 1.0\n"
     ]
    }
   ],
   "source": [
    "p = 0.5\n",
    "h = -np.log2(p)\n",
    "print(f'p(x)={p}, information: {h}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.linspace(0.1, 1.0, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Cross-Entropy for Machine Learning\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to entropy, cross entropy and KL divergence in machine learning\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - [An introduction to entropy, cross entropy and KL divergence in machine learning](https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "568px",
    "left": "301px",
    "top": "143px",
    "width": "245px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
