{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's an intuitive way to think of cross entropy\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - [What's an intuitive way to think of cross entropy](https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Information Entropy\n",
    "\n",
    "**Source:**\n",
    "\n",
    ">- [A Gentle Introduction to Information Entropy](https://machinelearningmastery.com/what-is-information-entropy/)\n",
    "\n",
    "## What is information theory\n",
    "\n",
    "Information theory is field of study concerned with quantifying information for communication.\n",
    "\n",
    "*信息论是涉及量化沟通信息的研究领域.*\n",
    "\n",
    "A foundational concept from information is the quantification of the amount of information in things like events, random variables, and distribution.\n",
    "\n",
    "*信息的一个基本概念是对诸如事件, 随机变量和分布的信息量的量化.*\n",
    "\n",
    "> *Why unify information theory and machine learning? Because they are two side of the ame coin. Information theory and machine learning still alone together.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the information for an event\n",
    "\n",
    "Quantifying information is the foundation of field of information theory.\n",
    "\n",
    "The intuition behind quantifying information is the idea of measuring how much surprise there is in an event. Those events that rare (low probability) are more surprising and therefore have more information those events that there are common (high probability).\n",
    "\n",
    "- **Low probability event:** High information (surprising).\n",
    "- **High probability event:** Low information (unsurprising)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Cross-Entropy for Machine Learning\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to entropy, cross entropy and KL divergence in machine learning\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - [An introduction to entropy, cross entropy and KL divergence in machine learning](https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "568px",
    "left": "301px",
    "top": "143px",
    "width": "245px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
